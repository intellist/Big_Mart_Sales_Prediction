# -*- coding: utf-8 -*-
"""bigmart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K5V1gk1PIXVoszUpLe84lAcmQcV_K1W2
"""



from google.colab import drive
drive.mount('/content/drive')

"""## Start"""



train_data_path = '/content/drive/MyDrive/Colab Notebooks/ABB_test/train_v9rqX0R.csv'
test_data_path = '/content/drive/MyDrive/Colab Notebooks/ABB_test/test_AbJTz2l.csv'

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv(train_data_path)
df.shape

df.columns

df.head(4)

df['Outlet_Identifier'].value_counts()

"""* There 12 columns.
* Item_Identifier, Outlet_Identifier and Item_Outlet_Sales are not relevant in training features.
* We have 9 columns to work with.

* Categorical cols: ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']
* Numerical cols: ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']
"""

df.columns

df.isna().sum()

df['Item_Fat_Content'].value_counts()

"""There are just 2 categories for fat content: low fat and regular."""

df['Item_Type'].value_counts()

df['Outlet_Size'].value_counts()

df['Outlet_Location_Type'].value_counts()

df['Outlet_Type'].value_counts()

df['Item_Visibility'].min(), df['Item_Visibility'].mean(),  df['Item_Visibility'].max()

df['Item_MRP'].min(), df['Item_MRP'].mean(), df['Item_MRP'].max()

df['Outlet_Establishment_Year'].min(), df['Outlet_Establishment_Year'].max()

df['Outlet_Establishment_Year'].value_counts()

year_counts = df['Outlet_Establishment_Year'].value_counts().sort_index()

# Plot
plt.figure(figsize=(10,6))
plt.bar(year_counts.index, year_counts.values)
plt.xlabel('Outlet Establishment Year')
plt.ylabel('Count')
plt.title('Number of Outlets Established Each Year')
plt.xticks(year_counts.index, rotation=45)
plt.show()

"""There are some years when no establishment opened."""





df.isna().sum()

df['Outlet_Identifier'].value_counts()

adict = {}
for ele in zip(df['Outlet_Identifier'], df['Outlet_Size']):
  adict[(ele[0], ele[1])] = 1

adict

# ('OUT049', 'Medium'): 1,
#  ('OUT018', 'Medium'): 1,
#  ('OUT010', nan): 1,-----------Grocery store, tier 3  (small)
#  ('OUT013', 'High'): 1,
#  ('OUT027', 'Medium'): 1,
#  ('OUT045', nan): 1,------------Supermarket 1, Tier2   (small)
#  ('OUT017', nan): 1,------------Supermarket 1, Tier2   (small)
#  ('OUT046', 'Small'): 1,
#  ('OUT035', 'Small'): 1,
#  ('OUT019', 'Small'): 1

df['Outlet_Size'].value_counts()

"""### Feature Engineering Operations"""

df['Outlet_Size'].value_counts()

df['Outlet_Size'].fillna('Small', inplace=True)

df['Item_Fat_Content'].unique()

df['Item_Fat_Content'] = df['Item_Fat_Content'].map({'Low Fat':'LF',
                            'Regular': 'reg',
                            'LF': 'LF',
                            'reg':'reg',
                            'low fat': 'LF'})
df['Item_Fat_Content'].value_counts()

df.columns

df['age'] = 2013 - df['Outlet_Establishment_Year']

df['Item_Weight'] = df['Item_Weight'].fillna(df.groupby('Item_Identifier')['Item_Weight'].transform('mean'))
overall_mean = df['Item_Weight'].mean()
df['Item_Weight'] = df['Item_Weight'].fillna(overall_mean)

df['price_per_weight'] = df['Item_MRP']/df['Item_Weight']

np.percentile(df['Item_Visibility'], 0), np.percentile(df['Item_Visibility'], 40), np.percentile(df['Item_Visibility'], 80)

df['Item_Visibility'].max()

df['visibility_level'] = pd.cut(df['Item_Visibility'],
                            bins=[-1, 0.0417543446, 0.1069244294, float('inf')],
                            labels=['Low', 'Medium', 'High'])

# df['weigh_level'] = pd.cut(df['Item_Weight'],
#                             bins=[-1, 10, 15, float('inf')],
#                             labels=['Low', 'Medium', 'High'])

df['weigh_level'] = pd.cut(df['Item_Weight'],
                            bins=[-1, 12.5, float('inf')],
                            labels=['Low', 'High'])

df['MRP_level'] = pd.cut(df['Item_MRP'],
                            bins=[-1, 100, 210, float('inf')],
                            labels=['Low', 'Medium', 'High'])

# df['MRP_level'] = pd.cut(df['Item_MRP'],
#                             bins=[-1, 70, 135, 200, float('inf')],
#                             labels=['Low', 'Medium1', 'Medium2', 'High'])

# 70, 135, 200

df['MRP_level'] .value_counts()

df['Item_MRP'].min(), df['Item_MRP'].max()

np.percentile(df['Item_MRP'], 50)

df['Item_Identifier_cat'] = df['Item_Identifier'].apply(lambda ele: ele[:2])
df['Item_Identifier_cat'].value_counts()

# df['Item_Type'].value_counts()

perishable_items = ['Fruits and Vegetables', 'Dairy', 'Meat', 'Breads', 'Seafood', 'Breakfast']

df['is_perishable'] = df['Item_Type'].isin(perishable_items).astype(int)

df['is_perishable'].value_counts()

df['visibility_mrp'] = df['Item_Visibility']*df['Item_MRP']
df['log_visibility'] = np.log(df['Item_Visibility'])
df['log_visibility_mrp'] = np.log(df['visibility_mrp'])

df['Item_Type'].value_counts()

processed_items = ['Snack Foods', 'Frozen Foods', 'Canned', 'Baking Goods', 'Soft Drinks',
                   'Breads', 'Hard Drinks', 'Starchy Foods']
df['is_processed'] = df['Item_Type'].isin(processed_items).astype(int)

# df.loc[df['Outlet_Type'] == 'Supermarket Type3', 'Outlet_Type'] = 'Supermarket Type2'

df['is_supermarket'] = df['Outlet_Type'].isin(['Supermarket Type1', 'Supermarket Type2', 'Supermarket Type3']).astype(int)

def map_item_type(x):
    if x in ['Fruits and Vegetables', 'Snack Foods', 'Frozen Foods', 'Canned', 'Baking Goods', 'Meat',
            'Breads', 'Starchy Foods', 'Breakfast', 'Seafood']:
        return 'food'
    elif x in ['Dairy', 'Soft Drinks', 'Hard Drinks']:
        return 'drinks'
    elif x in ['Household', 'Health and Hygiene', 'Others']:
        return 'non_eatable'
    else:
        return 'unknown'

# Apply the mapping
df['item_type_cat'] = df['Item_Type'].apply(map_item_type)

tier_map = {'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3}
df['Outlet_Tier'] = df['Outlet_Location_Type'].map(tier_map)

df['Visibility_Perishable'] = df['Item_Visibility'] * df['is_perishable']

df['Outlet_Location_Type'].value_counts()

# # Mean sales per Item
# df['item_avg_sales'] = df['Item_Identifier'].map(df.groupby('Item_Identifier')['Item_Outlet_Sales'].mean())

# # Mean sales per Outlet
# df['outlet_avg_sales'] = df['Outlet_Identifier'].map(df.groupby('Outlet_Identifier')['Item_Outlet_Sales'].mean())

from sklearn.model_selection import KFold

# Create empty feature
df['item_avg_sales'] = np.nan

# 5-fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, val_idx in kf.split(df):
    train_data, val_data = df.iloc[train_idx], df.iloc[val_idx]

    # Group on training data only
    means = train_data.groupby('Item_Identifier')['Item_Outlet_Sales'].mean()

    # Map means to validation data
    df.loc[val_idx, 'item_avg_sales'] = df.loc[val_idx, 'Item_Identifier'].map(means)

# Finally, fill remaining NaNs (if any) with global mean
df['item_avg_sales'].fillna(df['Item_Outlet_Sales'].mean(), inplace=True)

# Create empty feature
df['outlet_avg_sales'] = np.nan

# 5-fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, val_idx in kf.split(df):
    train_data, val_data = df.iloc[train_idx], df.iloc[val_idx]

    # Group on training data only
    means = train_data.groupby('Outlet_Identifier')['Item_Outlet_Sales'].mean()

    # Map means to validation data
    df.loc[val_idx, 'outlet_avg_sales'] = df.loc[val_idx, 'Outlet_Identifier'].map(means)

# Finally, fill remaining NaNs (if any) with global mean
df['outlet_avg_sales'].fillna(df['Item_Outlet_Sales'].mean(), inplace=True)



# # List of non-veg Item Types
# nonveg_items = ['Meat', 'Seafood']

# # Create if_nonveg column
# df['is_nonveg'] = df['Item_Type'].isin(nonveg_items).astype(int)

# df['outlet_type_size'] = df['Outlet_Type'] + "_" + df['Outlet_Size']

# df['outlet_type_size'].value_counts()

df['Outlet_Type'].value_counts()

df['Outlet_Size'].value_counts()

df.columns

# cat_features

# num_features = set(df.columns) - set(cat_features)
# num_features

"""* Only Item_Weight and Outlet_Size has missing values"""

df.columns

df['Item_Identifier'].nunique()

df['Outlet_Identifier'].nunique()

df['Item_Weight'].min(),df['Item_Weight'].mean(),  df['Item_Weight'].max()

import seaborn as sns

plt.figure(figsize=(8,5))
sns.histplot(df['Item_Weight'], kde=True, bins=30, color='blue')
# plt.title('Original Distribution of Item_Outlet_Sales')
plt.xlabel('Item_Weight')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8,5))
sns.histplot(df['Item_MRP'], kde=True, bins=40, color='blue')
# plt.title('Original Distribution of Item_Outlet_Sales')
plt.xlabel('Item_MRP')
plt.ylabel('Frequency')
plt.show()

counts, bin_edges = np.histogram(df['Item_MRP'], bins=40)

counts

bin_edges

plt.figure(figsize=(8,5))
sns.histplot(df['Item_Visibility'], kde=True, bins=30, color='blue')
# plt.title('Original Distribution of Item_Outlet_Sales')
plt.xlabel('Item_Visibility')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8,5))
sns.histplot(np.log(df['Item_Visibility']), kde=True, bins=30, color='blue')
# plt.title('Original Distribution of Item_Outlet_Sales')
plt.xlabel('Item_Visibility')
plt.ylabel('Frequency')
plt.show()

df['log_visibility_level'] = pd.cut(df['log_visibility'],
                            bins=[-float('inf'), -4.419374249183716, -2.688863260857491, float('inf')],
                            labels=['Low', 'Medium', 'High'])
df['log_visibility_level'].fillna('Low', inplace=True)

df['log_visibility_level'].isna().sum()

np.percentile(df['log_visibility'], 10), np.percentile(df['log_visibility'], 60)

plt.figure(figsize=(8,5))
sns.histplot(df['visibility_mrp'], kde=True, bins=30, color='blue')
# plt.title('Original Distribution of Item_Outlet_Sales')
plt.xlabel('visibility_mrp')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8,5))
sns.histplot(np.log(df['visibility_mrp']), kde=True, bins=30, color='blue')
# plt.title('Original Distribution of Item_Outlet_Sales')
plt.xlabel('visibility_mrp')
plt.ylabel('Frequency')
plt.show()

# numerical_features = set(feats) - set(cat_features)
# numerical_features

corr = df[list(numerical_features) + ['Item_Outlet_Sales']].corr()

# Set up the matplotlib figure
plt.figure(figsize=(12, 8))

# Draw the heatmap
sns.heatmap(corr,
            annot=True,        # show the correlation values
            fmt=".2f",          # format to 2 decimal places
            cmap='coolwarm',    # color map
            linewidths=0.5,     # lines between boxes
            square=True,        # make squares
            cbar_kws={"shrink": .8})  # color bar shrink
plt.title('Correlation Heatmap', fontsize=16)
plt.show()

# to remove
# Item_Visibility
# log_visibility_mrp

cat_features

df['Outlet_Type'].value_counts()

# Supermarket Type2 ---- medium, tier 3
# Supermarket Type3 ---- medium, tier 3

df[df['Outlet_Type'] == 'Grocery Store']



df.columns





"""## Train"""



!pip install catboost

df.columns

df.columns

ffeats = ['Item_Weight',
'Item_Fat_Content',
'Item_Type',
'Item_MRP',
'age',
'Outlet_Size',
'Outlet_Location_Type',
'Outlet_Type',
'MRP_level',
'weigh_level',
'log_visibility_level',
'price_per_weight',
'Item_Visibility',
'log_visibility',
'Outlet_Identifier',
'Item_Identifier_cat',
'is_perishable',
'visibility_mrp',
'is_processed',
'is_supermarket',
'item_type_cat',
'log_visibility_mrp',
'Outlet_Tier',
'item_avg_sales',
'outlet_avg_sales'
]

[ele for ele in ffeats]

from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

feats = ['Item_Weight',
         'Item_Fat_Content',
         'Item_Type',
         'Item_MRP',
         'age',
         'Outlet_Size',
         'Outlet_Location_Type',
         'Outlet_Type',
         'MRP_level',
        #  'weigh_level',

        #  'log_visibility_level',
        #  'price_per_weight',
        #  'Item_Visibility',
          # 'log_visibility',


        #  'Outlet_Identifier',


        #  'Item_Identifier_cat',
        #  'is_perishable',
        #  'visibility_mrp',

        #  'is_processed',
        #  'is_supermarket',
        #  'item_type_cat',

        #  'log_visibility_mrp',

        #  'Outlet_Tier',
        #  'item_avg_sales',
        #  'outlet_avg_sales'
         ]
len(feats)

# Features and target
X = df[feats]
y = df['Item_Outlet_Sales']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# X_train = X
# y_train = y



# ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']

# 'outlet_type_size' not good

# Identify categorical columns (by column names or indices)
cat_features = ['Item_Fat_Content',
                'Item_Type',
                'Outlet_Size',
                'Outlet_Location_Type',
                'Outlet_Type',
                # 'Outlet_Identifier',
                'MRP_level',
                # 'weigh_level',
                # 'log_visibility_level',
                # 'Outlet_Tier',
                # 'Item_Identifier_cat',
                # 'is_perishable',
                # 'is_processed', 'is_supermarket',
                # 'item_type_cat',
                ]


# Model
model = CatBoostRegressor(
    iterations=1000,
    learning_rate=0.05,
    depth=5,
    loss_function='RMSE',
    eval_metric='RMSE',
    random_seed=42,
    verbose=100
)
# # Best Parameters: {'depth': 4, 'learning_rate': 0.1, 'iterations': 500}
# model.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_test, y_test))



# Train
model.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_test, y_test))
# model.fit(X_train, y_train, cat_features=cat_features)

# Predict
y_pred = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")



from catboost import CatBoostRegressor, Pool

# Define parameter grid
param_grid = {
    'depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'iterations': [500, 1000, 1500],
    # 'l2_leaf_reg': [1, 3, 5, 7],
}

# Initialize model
model = CatBoostRegressor(
    loss_function='RMSE',
    random_seed=42,
    verbose=100
)

# Fit with grid search
result = model.grid_search(
    param_grid,
    X=Pool(X_train, y_train, cat_features=cat_features),
    cv=3,  # 3-fold cross-validation
    partition_random_seed=42,
    shuffle=True
)

print("Best Parameters:", result['params'])





# # perf
# Mean Squared Error: 1035425.7889
# Mean Squared Error: 1034132.5728  (after grouping fat content)
# Mean Squared Error: 1038030.5063  (after grouping fat content, adding age)
# Mean Squared Error: 1035907.9979  (after grouping fat content, adding age, price_per_weight)
# Mean Squared Error: 1037155.2055  (after grouping fat content, adding age, price_per_weight, fill na weight)



"""## Test"""

test_data_path = '/content/drive/MyDrive/Colab Notebooks/ABB_test/test_AbJTz2l.csv'
test_df = pd.read_csv(test_data_path)
test_df.shape

"""### Operations"""

# Step 1: Compute group means
train_means = df.groupby('Item_Identifier')['Item_Weight'].mean()
test_means = test_df.groupby('Item_Identifier')['Item_Weight'].mean()

# Step 2: Fill using train means first
test_df['Item_Weight'] = test_df['Item_Weight'].fillna(test_df['Item_Identifier'].map(train_means))

# Step 3: Fill using test means if train mean wasn't available
test_df['Item_Weight'] = test_df['Item_Weight'].fillna(test_df['Item_Identifier'].map(test_means))

# Step 4: Fill any remaining NaNs with overall mean (from train)
overall_mean = df['Item_Weight'].mean()
test_df['Item_Weight'] = test_df['Item_Weight'].fillna(overall_mean)


########################
test_df['Outlet_Size'].fillna('Small', inplace=True)
###########################
test_df['Item_Fat_Content'] = test_df['Item_Fat_Content'].map({'Low Fat':'LF',
                            'Regular': 'reg',
                            'LF': 'LF',
                            'reg':'reg',
                            'low fat': 'LF'})
test_df['Item_Fat_Content'].value_counts()
#################################
test_df['age'] = 2013 - test_df['Outlet_Establishment_Year']
test_df['price_per_weight'] = test_df['Item_MRP']/df['Item_Weight']
#####################################
test_df['MRP_level'] = pd.cut(test_df['Item_MRP'],
                            bins=[-1, 100, 210, float('inf')],
                            labels=['Low', 'Medium', 'High'])
# test_df['MRP_level'] = pd.cut(test_df['Item_MRP'],
#                             bins=[-1, 70, 135, 200, float('inf')],
#                             labels=['Low', 'Medium1', 'Medium2', 'High'])
######################################
test_df['weigh_level'] = pd.cut(test_df['Item_Weight'],
                            bins=[-1, 12.5, float('inf')],
                            labels=['Low', 'High'])
#################################################
test_df['Item_Identifier_cat'] = test_df['Item_Identifier'].apply(lambda ele: ele[:2])
test_df['Item_Identifier_cat'].value_counts()
##########################################
test_df['is_perishable'] = test_df['Item_Type'].isin(perishable_items).astype(int)
test_df['is_perishable'].value_counts()
##################################################
test_df['visibility_mrp'] = test_df['Item_Visibility']*test_df['Item_MRP']
test_df['outlet_type_size'] = test_df['Outlet_Type'] + "_" + test_df['Outlet_Size']
test_df['log_visibility'] = np.log(test_df['Item_Visibility'])
test_df['log_visibility_mrp'] = np.log(test_df['visibility_mrp'])
###########################################
test_df['is_processed'] = test_df['Item_Type'].isin(processed_items).astype(int)
##############################################
# test_df.loc[test_df['Outlet_Type'] == 'Supermarket Type3', 'Outlet_Type'] = 'Supermarket Type2'
#####################################
test_df['is_supermarket'] = test_df['Outlet_Type'].isin(['Supermarket Type1', 'Supermarket Type2', 'Supermarket Type3']).astype(int)
##############################
test_df['item_type_cat'] = test_df['Item_Type'].apply(map_item_type)
#######################################
test_df['Outlet_Tier'] = test_df['Outlet_Location_Type'].map(tier_map)
#####################################
test_df['Visibility_Perishable'] = test_df['Item_Visibility'] * test_df['is_perishable']
####################################
test_df['visibility_level'] = pd.cut(test_df['Item_Visibility'],
                            bins=[-1, 0.0417543446, 0.1069244294, float('inf')],
                            labels=['Low', 'Medium', 'High'])
#####################################################
# After creating item_avg_sales on train:
# (this 'means' must be created on full training data)

means = df.groupby('Item_Identifier')['Item_Outlet_Sales'].mean()
test_df['item_avg_sales'] = test_df['Item_Identifier'].map(means)

global_mean = df['Item_Outlet_Sales'].mean()

test_df['item_avg_sales'].fillna(global_mean, inplace=True)


means = df.groupby('Outlet_Identifier')['Item_Outlet_Sales'].mean()
test_df['outlet_avg_sales'] = test_df['Outlet_Identifier'].map(means)

global_mean = df['Item_Outlet_Sales'].mean()

test_df['outlet_avg_sales'].fillna(global_mean, inplace=True)

sample_submission_path = '/content/drive/MyDrive/Colab Notebooks/ABB_test/sample_submission_8RXa3c6.csv'
sub = pd.read_csv(sample_submission_path)

X_test_f = test_df[feats]
y_pred = model.predict(X_test_f)
final_sub = test_df[['Item_Identifier',	'Outlet_Identifier']]
final_sub.head(2)
final_sub['Item_Outlet_Sales'] = y_pred

final_sub['Item_Outlet_Sales'].min(), final_sub['Item_Outlet_Sales'].max()

final_sub.loc[final_sub['Item_Outlet_Sales'] < 0, 'Item_Outlet_Sales'] = 0.1
final_sub.to_csv('submission.csv', index=False)

for ele in feats:
  print(ele)



